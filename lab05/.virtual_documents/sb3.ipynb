





import gymnasium as gym
import matplotlib.pyplot as plt
from stable_baselines3 import SAC, PPO, A2C








# Pętla uczenia
for i in range(10):
    env = gym.make("BipedalWalker-v3")
    
    model = PPO("MlpPolicy", env, verbose=1, tensorboard_log="./a2c_bipedalwalker_tensorboard/", gamma=0.995)
    model.learn(total_timesteps=50_000)
    
    # rewards = []
    # infos = []
    # 
    # vec_env = model.get_env()
    # obs = vec_env.reset()
    





# Środowisko BipedalWalker
env_99 = gym.make("BipedalWalker-v3", render_mode="rgb_array")
model_99 = PPO("MlpPolicy", env, verbose=1, tensorboard_log="./a2c_bipedalwalker_tensorboard/", gamma=0.99)

# Pętla uczenia
for i in range(10):
    model_99.learn(total_timesteps=50_000)

# Let's visualize the agent's behavior


vec_env_99 = model_99.get_env()
obs_99 = vec_env_99.reset()





# Środowisko BipedalWalker
env_95 = gym.make("BipedalWalker-v3", render_mode="rgb_array")
model_95 = PPO("MlpPolicy", env, verbose=1, tensorboard_log="./a2c_bipedalwalker_tensorboard/", gamma=0.95)

# Pętla uczenia
for i in range(10):
    model_95.learn(total_timesteps=50_000)
    
# Let's visualize the agent's behavior


vec_env_95 = model_95.get_env()
obs_95 = vec_env_95.reset()





rewards = []
infos = []

for i in range(5000):
    action, _state = model.predict(obs, deterministic=True)
    obs, reward, done, info = env.step(action)
    env.render("human")
    print(reward, info)
    # saving rewards and info
    rewards.append(reward)
    infos.append(info)

# Plotting the rewards
plt.plot(list(rewards))
plt.xlabel("Time steps")
plt.ylabel("Reward")
plt.title("Reward over time")
plt.show()
